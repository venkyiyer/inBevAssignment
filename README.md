# inBevAssignment

# Assignment-1

### app.py : Execute 'uvicorn app:app'
### util.py : To add more documents to the vector store
### requirements.txt : All the necessary requirements
### Dockerfile: For Docker build

### Question and answers

##### My chunking strategy was to divide the document into character chunks of 1000 and an overlap of 500 characters with character splitter '\n'. I believe semantic chunking would work better as it chunks the document with respect to context and meaning based groups. Semantic chunking uses word level embeddings to understand the context between two chunks and groups them basis on their similarity.  

##### We can achieve faster parsing of documents by parallel batch processing, use faster inference embedding models, use libraries like NLTK, Spacy, for documents with simple pattern we can use regex. 

# Working screenshots

### Working screenshots with FastAPI endpoint
### Working Docker run but inaccessible localhost

# Assignment-3

### workflow-execution.md : Understanding of the paper


